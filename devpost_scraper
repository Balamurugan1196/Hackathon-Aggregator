from scraper_utils import get_driver, get_mongo_client, By, WebDriverWait, EC, datetime, re, Keys
import logging
import time

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def run_devpost_scraper():
    """Scrapes hackathon data from Devpost and stores it in MongoDB."""
    
    logging.info("Starting Devpost scraper...")

    # Initialize MongoDB connection
    db = get_mongo_client()
    collection = db["hackathons"]

    # Initialize Selenium WebDriver
    driver = get_driver()
    driver.get("https://devpost.com/hackathons")

    # Ensure Initial Content Loads
    WebDriverWait(driver, 15).until(EC.presence_of_all_elements_located((By.CLASS_NAME, "hackathon-tile")))

    # Dynamic Scrolling to Load More Hackathons
    TARGET_COUNT = 100  # Adjust as needed
    scroll_attempts, max_attempts = 0, 30
    prev_count = 0

    while True:
        events = driver.find_elements(By.CLASS_NAME, "hackathon-tile")
        current_count = len(events)

        if current_count >= TARGET_COUNT:
            logging.info(f"‚úÖ Loaded {current_count} hackathons. Stopping scroll.")
            break

        if current_count == prev_count:
            scroll_attempts += 1
            if scroll_attempts >= max_attempts:
                logging.warning("‚ö†Ô∏è No more hackathons found. Stopping.")
                break

        prev_count = current_count

        # Scroll Down using PAGE_DOWN multiple times before scrolling into view
        for _ in range(3):
            driver.find_element(By.TAG_NAME, "body").send_keys(Keys.PAGE_DOWN)
            time.sleep(1)

        if events:
            driver.execute_script("arguments[0].scrollIntoView();", events[-1])

        time.sleep(3)  # Allow time for new hackathons to load
        logging.info(f"üîÑ Scroll Attempt {scroll_attempts}: Found {current_count} hackathons.")

    scraped_events = []
    for event in events[:TARGET_COUNT]:
        try:
            driver.execute_script("arguments[0].scrollIntoView();", event)
            time.sleep(1)
            name = event.find_element(By.CSS_SELECTOR, "h3.mb-4").text
            date_text = event.find_element(By.CLASS_NAME, "submission-period").text  
            start_date, end_date = extract_dates(date_text)

            # Extract Mode & Location
            location_info = event.find_element(By.CLASS_NAME, "info").text
            mode = "Online" if "online" in location_info.lower() else "Offline"
            location = "None" if mode == "Online" else location_info

            # Extract Prize Money
            prize = extract_prize_money(event)

            # Extract Apply Link
            apply_link = event.find_element(By.TAG_NAME, "a").get_attribute("href")

            scraped_events.append({
                "name": name,
                "start_date": start_date,
                "end_date": end_date,
                "mode": mode,
                "location": location,
                "prize_money": prize,
                "apply_link": apply_link,
                "Source": "Devpost"
            })
        except Exception as e:
            logging.error(f"Skipping one event due to error: {e}")

    # Insert Data into MongoDB
    if scraped_events:
        collection.insert_many(scraped_events)
        logging.info(f"{len(scraped_events)} hackathons stored in MongoDB successfully! üöÄ")

    driver.quit()
